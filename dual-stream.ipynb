{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "62ce9e03-b011-479d-9e38-87c72d01a126",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-08T04:11:05.101991Z",
     "iopub.status.busy": "2026-02-08T04:11:05.101716Z",
     "iopub.status.idle": "2026-02-08T04:11:05.119639Z",
     "shell.execute_reply": "2026-02-08T04:11:05.118570Z",
     "shell.execute_reply.started": "2026-02-08T04:11:05.101968Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from types import SimpleNamespace\n",
    "\n",
    "import utils\n",
    "from data_handler import DataHandler\n",
    "from genetic_optimizer import ChromosomeHelper, GeneticAlgorithm\n",
    "from trainer import retrain_and_evaluate_best_model\n",
    "\n",
    "def build_derived_ga_params(config):\n",
    "    \"\"\"\n",
    "    Dynamically builds the VARTYPE and VARBOUND lists for a unified chromosome\n",
    "    that contains genes for all possible streams.\n",
    "    \"\"\"\n",
    "    logger = logging.getLogger()\n",
    "    logger.info(\"Dynamically building unified GA parameters from dictionary config...\")\n",
    "\n",
    "    ns = config.network_structure\n",
    "\n",
    "\n",
    "    CONV_GENE_ORDER = ['is_active', 'filters', 'kernel_size', 'activation', 'padding', 'strides', 'use_bn', 'use_pool', 'pool_type']\n",
    "    POST_CONV_GENE_ORDER = ['global_pooling', 'use_dropout', 'dropout_rate']\n",
    "    LSTM_GENE_ORDER = ['is_active', 'units', 'activation', 'recurrent_dropout', 'use_bn', 'use_dropout', 'dropout_rate']\n",
    "    MLP_GENE_ORDER = ['is_active', 'use_bn', 'use_dense', 'units', 'activation', 'use_dropout', 'dropout_rate']\n",
    "    NETWORK_PARAMS_GENE_ORDER = ['l1_reg', 'learning_rate']\n",
    "    VARTYPE_MAP = {'dropout_rate': 'real', 'recurrent_dropout': 'real', 'l1_reg': 'real', 'learning_rate': 'real'}\n",
    "\n",
    "\n",
    "    varbound_conv_block = [getattr(ns.conv_block_bounds, key) for key in CONV_GENE_ORDER]\n",
    "    vartype_conv_block = [VARTYPE_MAP.get(key, 'int') for key in CONV_GENE_ORDER]\n",
    "    varbound_post_conv = [getattr(ns.post_conv_bounds, key) for key in POST_CONV_GENE_ORDER]\n",
    "    vartype_post_conv = [VARTYPE_MAP.get(key, 'int') for key in POST_CONV_GENE_ORDER]\n",
    "    varbound_lstm_block = [getattr(ns.lstm_block_bounds, key) for key in LSTM_GENE_ORDER]\n",
    "    vartype_lstm_block = [VARTYPE_MAP.get(key, 'int') for key in LSTM_GENE_ORDER]\n",
    "    vartype_mlp_block = [VARTYPE_MAP.get(key, 'int') for key in MLP_GENE_ORDER]\n",
    "    varbound_mlp_block = [getattr(ns.mlp_block_bounds, key) for key in MLP_GENE_ORDER]\n",
    "    varbound_network_params = [getattr(ns.network_params_bounds, key) for key in NETWORK_PARAMS_GENE_ORDER]\n",
    "    vartype_network_params = [VARTYPE_MAP.get(key, 'int') for key in NETWORK_PARAMS_GENE_ORDER]\n",
    "\n",
    "\n",
    "    config.derived_ga_params.VARBOUND_CONV1D = varbound_conv_block * ns.N_CONV_BLOCKS_1D + varbound_post_conv\n",
    "    config.derived_ga_params.VARTYPE_CONV1D = vartype_conv_block * ns.N_CONV_BLOCKS_1D + vartype_post_conv\n",
    "    config.derived_ga_params.VARBOUND_CONV2D = varbound_conv_block * ns.N_CONV_BLOCKS_2D + varbound_post_conv\n",
    "    config.derived_ga_params.VARTYPE_CONV2D = vartype_conv_block * ns.N_CONV_BLOCKS_2D + vartype_post_conv\n",
    "    config.derived_ga_params.VARBOUND_HP_LSTM = varbound_lstm_block * ns.N_LSTM_BLOCKS\n",
    "    config.derived_ga_params.VARTYPE_HP_LSTM = vartype_lstm_block * ns.N_LSTM_BLOCKS\n",
    "    config.derived_ga_params.VARBOUND_MLP_SHARED = varbound_mlp_block * ns.N_MLP_BLOCKS + varbound_network_params\n",
    "    config.derived_ga_params.VARTYPE_MLP_SHARED = vartype_mlp_block * ns.N_MLP_BLOCKS + vartype_network_params\n",
    "\n",
    "    logger.info(\"Unified GA parameters built successfully.\")\n",
    "    return config\n",
    "    \n",
    "def get_paper_config(dataname=\"AREM\", subjects_train=None, subjects_val=None, subjects_test=None):\n",
    "    \"\"\"\n",
    "    Sets up the configuration parameters exactly as described in the paper.\n",
    "    \"\"\"\n",
    "    config = SimpleNamespace(\n",
    "        general=SimpleNamespace(\n",
    "            DATANAME_PREFIX=dataname,\n",
    "            BASE_CSV_PATH=f\"data/{dataname}.csv\",\n",
    "            RESULTS_DIR=\"results\",\n",
    "            FS=20,\n",
    "            TRAIN_SUBJECTS=subjects_train or ['A1','A2','A3',],\n",
    "            VALID_SUBJECTS=subjects_val or ['A4','A5'],\n",
    "            TEST_SUBJECTS=subjects_test or ['A6']\n",
    "        ),\n",
    "        data_preprocessing=SimpleNamespace(\n",
    "            WINDOW_SIZE_SECONDS_OPTIONS=[0.25, 0.5],\n",
    "            OVERLAP_RATIO_OPTIONS=[0.25],\n",
    "            DATA_CACHE_DIR=\"cache_data\",\n",
    "            INITIAL_NPERSEG_DIVISOR=5\n",
    "        ),\n",
    "        ga=SimpleNamespace(\n",
    "            POPULATION_SIZE=50,\n",
    "            GENERATIONS=30,\n",
    "            MAX_CHILDREN_PER_GENERATION_FACTOR=0.4,\n",
    "            CROSSOVER_PROBABILITY=0.8,\n",
    "            MUTATION_PROBABILITY=0.2,\n",
    "            OPTIMIZATION_METRIC=\"f1_macro\",\n",
    "            N_TRIALS_PER_EVALUATION=1,\n",
    "            EPOCHS_PER_TRIAL=10,\n",
    "            FITNESS_VERBOSE_LEVEL=1,\n",
    "            PATIENCE_EARLY_STOPPING=5,\n",
    "            BATCH_SIZES_OPTIONS=[64, 128, 256, 512, 1024],\n",
    "            ACTIVE_STREAMS=SimpleNamespace(\n",
    "                cnn_1d=True,  # Time-domain stream\n",
    "                cnn_2d=True,  # Frequency-domain stream (STFT)\n",
    "                lstm=False    # Paper focused on CNN dual-stream\n",
    "            ),\n",
    "            DEV_RATIO=0.2 # Ratio of training used for internal GA evaluation\n",
    "        ),\n",
    "        network_structure=SimpleNamespace(\n",
    "            N_CONV_BLOCKS_1D=2,\n",
    "            N_CONV_BLOCKS_2D=2,\n",
    "            N_LSTM_BLOCKS=0,\n",
    "            N_MLP_BLOCKS=2,\n",
    "            # Search space bounds for CNN layers (Paper lines 107-111)\n",
    "            conv_block_bounds=SimpleNamespace(\n",
    "                is_active=[0, 1],\n",
    "                filters=[1, 500],\n",
    "                kernel_size=[1, 8],\n",
    "                activation=[0, 2], # ReLU, Tanh, Sigmoid\n",
    "                padding=[0, 1],    # Valid, Same\n",
    "                strides=[1, 2],\n",
    "                use_bn=[0, 1],\n",
    "                use_pool=[0, 1],\n",
    "                pool_type=[0, 1]   # Max, Average\n",
    "            ),\n",
    "            post_conv_bounds=SimpleNamespace(\n",
    "                global_pooling=[0, 1],\n",
    "                use_dropout=[0, 1],\n",
    "                dropout_rate=[0.4, 0.5]\n",
    "            ),\n",
    "            lstm_block_bounds=SimpleNamespace(\n",
    "                is_active=[0, 1],\n",
    "                units=[32, 256],\n",
    "                activation=[0, 2],\n",
    "                recurrent_dropout=[0.0, 0.5],\n",
    "                use_bn=[0, 1],\n",
    "                use_dropout=[0, 1],\n",
    "                dropout_rate=[0.1, 0.5]\n",
    "            ),\n",
    "            mlp_block_bounds=SimpleNamespace(\n",
    "                is_active=[0, 1],\n",
    "                use_bn=[0, 1],\n",
    "                use_dense=[1, 1],\n",
    "                units=[3, 1024],\n",
    "                activation=[0, 2],\n",
    "                use_dropout=[0, 1],\n",
    "                dropout_rate=[0.4, 0.5]\n",
    "            ),\n",
    "            network_params_bounds=SimpleNamespace(\n",
    "                l1_reg=[1e-6, 1e-2],\n",
    "                learning_rate=[1e-6, 1e-2]\n",
    "            )\n",
    "        ),\n",
    "        loss_function=SimpleNamespace(\n",
    "            GAMMA_OPTIONS=[1.0, 2.0, 3.0, 5.0],\n",
    "            CLASS_WEIGHT_MULTIPLIER_OPTIONS=[1, 2, 5, 10]\n",
    "        ),\n",
    "        mappings=SimpleNamespace(\n",
    "            ACTIVATION_MAP=[\"relu\", \"tanh\", \"sigmoid\"],\n",
    "            PADDING_MAP=[\"valid\", \"same\"]\n",
    "        ),\n",
    "        training=SimpleNamespace(TRAINING_TIMEOUT_SECONDS=300),\n",
    "        derived_ga_params=SimpleNamespace() # Populated at runtime\n",
    "    )\n",
    "    return config\n",
    "\n",
    "def main():\n",
    "    # 1. Setup Environment\n",
    "    config = get_paper_config()\n",
    "    run_dir = os.path.join(config.general.RESULTS_DIR, f\"paper_run_{utils.get_timestamp()}\")\n",
    "    logger = utils.setup_logger(run_dir, config.general.DATANAME_PREFIX)\n",
    "    \n",
    "    logger.info(\"Starting Automated Dual-Stream Network Design...\")\n",
    "\n",
    "    # 2. Data Preparation\n",
    "    # Note: DataHandler must implement subject-based splitting as per paper Section 2.2\n",
    "    data_handler = DataHandler(config.general.BASE_CSV_PATH, config, logger)\n",
    "    data_handler.load_data()\n",
    "    data_handler.preprocess_labels()\n",
    "    \n",
    "    # Pre-generate sliding window datasets for the GA search space\n",
    "    data_handler.generate_and_cache_datasets(\n",
    "        window_size_options=config.data_preprocessing.WINDOW_SIZE_SECONDS_OPTIONS,\n",
    "        overlap_ratio_options=config.data_preprocessing.OVERLAP_RATIO_OPTIONS,\n",
    "        fs=config.general.FS,\n",
    "        cache_dir=config.data_preprocessing.DATA_CACHE_DIR,\n",
    "        train_subjects=config.general.TRAIN_SUBJECTS,\n",
    "        valid_subjects=config.general.VALID_SUBJECTS,\n",
    "        test_subjects=config.general.TEST_SUBJECTS\n",
    "    )\n",
    "\n",
    "    # 3. Setup Genetic Algorithm\n",
    "    config = build_derived_ga_params(config)\n",
    "    \n",
    "    chromosome_helper = ChromosomeHelper(data_handler.num_classes, config, logger)\n",
    "    \n",
    "    ga = GeneticAlgorithm(\n",
    "        chromosome_helper=chromosome_helper,\n",
    "        config=config,\n",
    "        logger=logger,\n",
    "        run_dir=run_dir\n",
    "    )\n",
    "\n",
    "    # 4. Execute Neural Architecture Search (NAS)\n",
    "    best_individual, is_finished = ga.run()\n",
    "\n",
    "    # 5. Final Evaluation (Retrain on Train+Val, test on Test)\n",
    "    if is_finished:\n",
    "        logger.info(\"NAS complete. Retraining best model for final evaluation...\")\n",
    "        retrain_and_evaluate_best_model(\n",
    "            best_individual_parts=best_individual[0],\n",
    "            best_individual_fitness=best_individual[1],\n",
    "            config=config,\n",
    "            logger=logger,\n",
    "            run_dir=run_dir,\n",
    "            epochs_retrain=50 # Final convergence\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0520c8c1-8071-4801-98bd-b2d0ed9e5678",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-08T04:11:05.580633Z",
     "iopub.status.busy": "2026-02-08T04:11:05.580112Z",
     "iopub.status.idle": "2026-02-08T04:11:10.329774Z",
     "shell.execute_reply": "2026-02-08T04:11:10.328604Z",
     "shell.execute_reply.started": "2026-02-08T04:11:05.580583Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-08 07:41:05,582 - AREM - utils.py - INFO - TensorFlow version: 2.18.0\n",
      "2026-02-08 07:41:05,582 - AREM - utils.py - INFO - TensorFlow version: 2.18.0\n",
      "2026-02-08 07:41:05,585 - AREM - utils.py - INFO - Available GPUs: []\n",
      "2026-02-08 07:41:05,585 - AREM - utils.py - INFO - Available GPUs: []\n",
      "2026-02-08 07:41:05,586 - AREM - utils.py - WARNING - No GPU detected by TensorFlow. Running on CPU will be slow.\n",
      "2026-02-08 07:41:05,586 - AREM - utils.py - WARNING - No GPU detected by TensorFlow. Running on CPU will be slow.\n",
      "2026-02-08 07:41:05,587 - AREM - 2658416851.py - INFO - Starting Automated Dual-Stream Network Design...\n",
      "2026-02-08 07:41:05,587 - AREM - 2658416851.py - INFO - Starting Automated Dual-Stream Network Design...\n",
      "2026-02-08 07:41:05,588 - AREM - data_handler.py - INFO - Loading data from data/AREM.csv\n",
      "2026-02-08 07:41:05,588 - AREM - data_handler.py - INFO - Loading data from data/AREM.csv\n",
      "2026-02-08 07:41:05,643 - AREM - data_handler.py - INFO - Data loaded successfully. Shape: (41759, 8)\n",
      "2026-02-08 07:41:05,643 - AREM - data_handler.py - INFO - Data loaded successfully. Shape: (41759, 8)\n",
      "2026-02-08 07:41:05,649 - AREM - data_handler.py - INFO - Preprocessing labels (factorizing 'Class' column)\n",
      "2026-02-08 07:41:05,649 - AREM - data_handler.py - INFO - Preprocessing labels (factorizing 'Class' column)\n",
      "2026-02-08 07:41:05,662 - AREM - data_handler.py - INFO - Factorized 'Class' into 7 classes.\n",
      "2026-02-08 07:41:05,662 - AREM - data_handler.py - INFO - Factorized 'Class' into 7 classes.\n",
      "2026-02-08 07:41:05,663 - AREM - data_handler.py - INFO - Class mapping: [(0, 'bending1'), (1, 'bending2'), (2, 'cycling'), (3, 'lying'), (4, 'sitting'), (5, 'standing'), (6, 'walking')]\n",
      "2026-02-08 07:41:05,663 - AREM - data_handler.py - INFO - Class mapping: [(0, 'bending1'), (1, 'bending2'), (2, 'cycling'), (3, 'lying'), (4, 'sitting'), (5, 'standing'), (6, 'walking')]\n",
      "2026-02-08 07:41:05,664 - AREM - data_handler.py - INFO - --- Starting Pre-generation and Caching of Datasets into 'cache_data'  ---\n",
      "2026-02-08 07:41:05,664 - AREM - data_handler.py - INFO - --- Starting Pre-generation and Caching of Datasets into 'cache_data'  ---\n",
      "2026-02-08 07:41:05,665 - AREM - data_handler.py - INFO - Dataset already exists, skipping: data_ws_0.25s_ol_0.25.npz\n",
      "2026-02-08 07:41:05,665 - AREM - data_handler.py - INFO - Dataset already exists, skipping: data_ws_0.25s_ol_0.25.npz\n",
      "2026-02-08 07:41:05,666 - AREM - data_handler.py - INFO - Dataset already exists, skipping: data_ws_0.5s_ol_0.25.npz\n",
      "2026-02-08 07:41:05,666 - AREM - data_handler.py - INFO - Dataset already exists, skipping: data_ws_0.5s_ol_0.25.npz\n",
      "2026-02-08 07:41:05,667 - AREM - data_handler.py - INFO - --- Finished Caching All Datasets ---\n",
      "2026-02-08 07:41:05,667 - AREM - data_handler.py - INFO - --- Finished Caching All Datasets ---\n",
      "2026-02-08 07:41:05,668 - AREM - genetic_optimizer.py - INFO - Starting Genetic Algorithm...\n",
      "2026-02-08 07:41:05,668 - AREM - genetic_optimizer.py - INFO - Starting Genetic Algorithm...\n",
      "2026-02-08 07:41:05,670 - AREM - genetic_optimizer.py - INFO - Configuration saved to results/paper_run_20260208-074105/config.json\n",
      "2026-02-08 07:41:05,670 - AREM - genetic_optimizer.py - INFO - Configuration saved to results/paper_run_20260208-074105/config.json\n",
      "2026-02-08 07:41:05,673 - AREM - genetic_optimizer.py - INFO - No checkpoint file found. Starting a new GA run.\n",
      "2026-02-08 07:41:05,673 - AREM - genetic_optimizer.py - INFO - No checkpoint file found. Starting a new GA run.\n",
      "2026-02-08 07:41:05,675 - AREM - genetic_optimizer.py - INFO - Initializing population of size 50\n",
      "2026-02-08 07:41:05,675 - AREM - genetic_optimizer.py - INFO - Initializing population of size 50\n",
      "2026-02-08 07:41:05,677 - AREM - genetic_optimizer.py - INFO - Loaded sample data for initialization from cache_data/data_ws_0.25s_ol_0.25.npz\n",
      "2026-02-08 07:41:05,677 - AREM - genetic_optimizer.py - INFO - Loaded sample data for initialization from cache_data/data_ws_0.25s_ol_0.25.npz\n",
      "2026-02-08 07:41:05,707 - AREM - genetic_optimizer.py - WARNING - Initial chromosome failed build check. Retrying generation.\n",
      "2026-02-08 07:41:05,707 - AREM - genetic_optimizer.py - WARNING - Initial chromosome failed build check. Retrying generation.\n",
      "2026-02-08 07:41:05,879 - AREM - model_architectures.py - INFO - Model built and compiled successfully.\n",
      "2026-02-08 07:41:05,879 - AREM - model_architectures.py - INFO - Model built and compiled successfully.\n",
      "2026-02-08 07:41:06,000 - AREM - genetic_optimizer.py - INFO - Successfully loaded data from cache_data/data_ws_0.25s_ol_0.25.npz\n",
      "2026-02-08 07:41:06,000 - AREM - genetic_optimizer.py - INFO - Successfully loaded data from cache_data/data_ws_0.25s_ol_0.25.npz\n",
      "2026-02-08 07:41:06,020 - AREM - genetic_optimizer.py - INFO - Fitness trial 1/1\n",
      "2026-02-08 07:41:06,020 - AREM - genetic_optimizer.py - INFO - Fitness trial 1/1\n",
      "2026-02-08 07:41:06,210 - AREM - model_architectures.py - INFO - Model built and compiled successfully.\n",
      "2026-02-08 07:41:06,210 - AREM - model_architectures.py - INFO - Model built and compiled successfully.\n",
      "2026-02-08 07:41:06,212 - AREM - trainer.py - INFO - Using 1D data stream (shape: (503, 5, 6))\n",
      "2026-02-08 07:41:06,212 - AREM - trainer.py - INFO - Using 1D data stream (shape: (503, 5, 6))\n",
      "2026-02-08 07:41:06,215 - AREM - trainer.py - INFO - Using 2D data stream (shape: (503, 1, 5, 6))\n",
      "2026-02-08 07:41:06,215 - AREM - trainer.py - INFO - Using 2D data stream (shape: (503, 1, 5, 6))\n",
      "2026-02-08 07:41:06,216 - AREM - trainer.py - INFO - Starting training: epochs=1000, batch_size=128\n",
      "2026-02-08 07:41:06,216 - AREM - trainer.py - INFO - Starting training: epochs=1000, batch_size=128\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 108ms/step - accuracy: 0.3833 - f1_score_macro: 0.3500 - loss: 8.8278 - val_accuracy: 0.1418 - val_f1_score_macro: 0.0355 - val_loss: 16.6969\n",
      "Epoch 2/1000\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - accuracy: 0.6544 - f1_score_macro: 0.6178 - loss: 3.1114 - val_accuracy: 0.1710 - val_f1_score_macro: 0.0740 - val_loss: 22.2536\n",
      "Epoch 3/1000\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - accuracy: 0.7416 - f1_score_macro: 0.7390 - loss: 2.4284 - val_accuracy: 0.2759 - val_f1_score_macro: 0.1857 - val_loss: 20.6690\n",
      "Epoch 4/1000\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - accuracy: 0.7985 - f1_score_macro: 0.7807 - loss: 1.5251 - val_accuracy: 0.2801 - val_f1_score_macro: 0.1789 - val_loss: 21.9553\n",
      "Epoch 5/1000\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - accuracy: 0.7518 - f1_score_macro: 0.7278 - loss: 1.5479 - val_accuracy: 0.2503 - val_f1_score_macro: 0.1565 - val_loss: 23.4854\n",
      "Epoch 6/1000\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - accuracy: 0.8666 - f1_score_macro: 0.8700 - loss: 1.3032 - val_accuracy: 0.2354 - val_f1_score_macro: 0.1204 - val_loss: 23.5623\n",
      "Epoch 7/1000\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - accuracy: 0.8521 - f1_score_macro: 0.8368 - loss: 1.1577 - val_accuracy: 0.2330 - val_f1_score_macro: 0.1366 - val_loss: 25.2304\n",
      "Epoch 8/1000\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - accuracy: 0.7618 - f1_score_macro: 0.7270 - loss: 1.3192 - val_accuracy: 0.2765 - val_f1_score_macro: 0.1801 - val_loss: 23.3484\n",
      "Epoch 9/1000\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - accuracy: 0.8924 - f1_score_macro: 0.8913 - loss: 1.0993 - val_accuracy: 0.3230 - val_f1_score_macro: 0.2302 - val_loss: 20.4575\n",
      "Epoch 10/1000\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - accuracy: 0.8691 - f1_score_macro: 0.8704 - loss: 1.0309 - val_accuracy: 0.3564 - val_f1_score_macro: 0.2573 - val_loss: 19.8451\n",
      "Epoch 11/1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <bound method IPythonKernel._clean_thread_parent_frames of <ipykernel.ipkernel.IPythonKernel object at 0x7fa30c19cc70>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/conficker/anaconda3/envs/tf218/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 775, in _clean_thread_parent_frames\n",
      "    def _clean_thread_parent_frames(\n",
      "KeyboardInterrupt: \n",
      "Exception ignored in: <function WeakKeyDictionary.__init__.<locals>.remove at 0x7fa2820463b0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/conficker/anaconda3/envs/tf218/lib/python3.10/weakref.py\", line 370, in remove\n",
      "    def remove(k, selfref=ref(self)):\n",
      "KeyboardInterrupt: \n",
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76d64dff-5a38-48dc-b206-30eb5413092a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
